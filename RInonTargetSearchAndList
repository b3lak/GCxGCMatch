import sys
import pandas as pd
import numpy as np
from PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QVBoxLayout, QFileDialog, QLabel, QMessageBox
from scipy.stats import mannwhitneyu

class ExcelProcessor(QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()
        
    def initUI(self):
        self.setWindowTitle("Excel Processor")
        self.setGeometry(200, 200, 400, 300)
        
        layout = QVBoxLayout()

        self.label = QLabel('Select an Excel file to start.')
        layout.addWidget(self.label)
        
        self.browse_button = QPushButton('Browse Input File', self)
        self.browse_button.clicked.connect(self.browse_file)
        layout.addWidget(self.browse_button)
        
        self.ok_button = QPushButton('OK', self)
        self.ok_button.clicked.connect(self.process_all)
        layout.addWidget(self.ok_button)

        self.setLayout(layout)
        
        self.show()

    def browse_file(self):
        options = QFileDialog.Options()
        file_name, _ = QFileDialog.getOpenFileName(self, "Select Excel File", "", "Excel Files (*.xlsx);;All Files (*)", options=options)
        if file_name:
            self.input_file_path = file_name
            self.label.setText(f'Selected: {file_name}')
  
    def generate_master_list(self):
        if not hasattr(self, 'input_file_path'):
            QMessageBox.warning(self, "Error", "Please select an Excel file first.")
            return
        #Print statement for personal viewing
        print("Generating Master List")
        excel_file = pd.ExcelFile(self.input_file_path)
        sheet_names = excel_file.sheet_names

        tolerances = {'RT1': 10, 'RT2': 10, 'Major': 0.12, 'Qual': 0.12}
        df_all = pd.read_excel(excel_file, sheet_name=sheet_names[0])

        for sheet in sheet_names[1:]:
            df = pd.read_excel(excel_file, sheet_name=sheet)
            masks = [not (
                np.isclose(df_all['RT1'], row['RT1'], atol=tolerances['RT1']) &
                np.isclose(df_all['RT2'], row['RT2'], atol=tolerances['RT2']) &
                np.isclose(df_all['Major'], row['Major'], atol=tolerances['Major']) &
                np.isclose(df_all['Qual'], row['Qual'], atol=tolerances['Qual'])
            ).any() for _, row in df.iterrows()]
            df_new = df[masks]
            df_all = pd.concat([df_all, df_new], ignore_index=True)

        self.df_master = df_all

        print("Master list generated now doing the search")

    def process_file(self):
        if not hasattr(self, 'df_master'):
            QMessageBox.warning(self, "Error", "Please generate the master list first.")
            return
        print("Beggining the Search")
        # The logic from the second script for comparison starts here:
        tolerances = {
            'RT1': 25,
            'RT2': 25,
            'Major': 0.12
        }

        df_first = self.df_master
        first_sheet = "Master"
        xls = pd.ExcelFile(self.input_file_path)

        df_first.columns = [f"{col}_{first_sheet}" for col in df_first.columns]

        result_dfs = {
            "Area": df_first[["Compound_" + first_sheet, "RT1_" + first_sheet, "RT2_" + first_sheet, "Major_" + first_sheet]].copy()
        }

        for sheet_name in xls.sheet_names:
            df = pd.read_excel(xls, sheet_name)
            df.columns = [f"{col}_{sheet_name}" for col in df.columns]

            for index, row in df_first.iterrows():
                base_conditions = (
                    df[f"RT1_{sheet_name}"].between(row[f"RT1_{first_sheet}"] - tolerances['RT1'], row[f"RT1_{first_sheet}"] + tolerances['RT1']) &
                    df[f"RT2_{sheet_name}"].between(row[f"RT2_{first_sheet}"] - tolerances['RT2'], row[f"RT2_{first_sheet}"] + tolerances['RT2']) &
                    df[f"Major_{sheet_name}"].between(row[f"Major_{first_sheet}"] - tolerances['Major'], row[f"Major_{first_sheet}"] + tolerances['Major'])
                )

                # Area and Area%
                matched_row_area = df[base_conditions]["Area_" + sheet_name].values
                matched_row_area_pct = df[base_conditions]["Area %" + "_" + sheet_name].values
                if matched_row_area.any():
                    result_dfs["Area"].loc[index, f"Area_{sheet_name}"] = matched_row_area[0]
                    result_dfs["Area"].loc[index, f"Area %_{sheet_name}"] = matched_row_area_pct[0] if matched_row_area_pct.any() else None
                else:
                    result_dfs["Area"].loc[index, f"Area_{sheet_name}"] = None
                    result_dfs["Area"].loc[index, f"Area %_{sheet_name}"] = None

        # Saving the results
        df_output = result_dfs["Area"]
        
        #VARIENCE CHECKING
        print("Search complete. Beggining Varience Check")
        #print("First four column names:", df_output.columns[:4].tolist())

        # Fill NaN values in 'Compound_Results' with 'Compound_index+1'
        df_output['Compound_Master'] = df_output['Compound_Master'].combine_first(df_output.index.to_series().add(1).astype(str).radd('Compound_'))

        # Calculate variance for each sample for each compound
        all_vars, all_means, all_stds = [], [], []

        for i in range(1, 34):
            var_col_name = f"Variance_S{i}"
            mean_col_name = f"Mean_S{i}"
            std_dev_col_name = f"StdDev_S{i}"
            
            current_var = df_output[[f"Area_S{i}-1", f"Area_S{i}-2", f"Area_S{i}-3"]].var(axis=1).rename(var_col_name)
            current_mean = df_output[[f"Area_S{i}-1", f"Area_S{i}-2", f"Area_S{i}-3"]].mean(axis=1).rename(mean_col_name)
            current_std = df_output[[f"Area_S{i}-1", f"Area_S{i}-2", f"Area_S{i}-3"]].std(axis=1).rename(std_dev_col_name)
            
            all_vars.append(current_var)
            all_means.append(current_mean)
            all_stds.append(current_std)

        # Concatenate all new columns at once
        additional_cols = pd.concat([*all_vars, *all_means, *all_stds], axis=1)
        df_output = pd.concat([df_output, additional_cols], axis=1)

        # Aggregate variance using mean across all samples for each compound
        df_output['Avg_Variance'] = df_output[[f"Variance_S{i}" for i in range(1, 34)]].mean(axis=1)

        # Set the threshold for aggregated variance
        threshold = 1e+13

        # Filter out compounds with average variance above the threshold
        high_variance_compounds = df_output[df_output['Avg_Variance'] > threshold]

        # Print the number and details of compounds that will be removed
        print("Number of compounds with high variance:", len(high_variance_compounds))

        # can be un tabbed if you want the removed high variance compounds to be displayed
        #print("Details of compounds to be removed:")
        #print(high_variance_compounds[['Compound_Master', 'Avg_Variance']])

        # Remove the compounds with high variance from df_output
        df_output = df_output[df_output['Avg_Variance'] <= threshold]

        # Starting Mann-Whitney U test

        significant_count = 0

        # Loop through each station
        for i in range(1, 34):
            
            # Track significant tests per station
            station_significance_count = 0
            
            # Loop through each compound
            for _, row in df_output.iterrows():
                # Extract replicate values for current station and compound
                replicates_1_2 = [row[f"Area_S{i}-1"], row[f"Area_S{i}-2"]]  # Values from first two replicates
                replicate_3 = row[f"Area_S{i}-3"]  # Value from third replicate
                
                # Removing NaN values from replicates_1_2
                replicates_1_2 = [x for x in replicates_1_2 if not pd.isna(x)]
                
                # Only perform test if there's non-NaN data for comparison
                if len(replicates_1_2) > 0 and not pd.isna(replicate_3):
                    _, p_val = mannwhitneyu(replicates_1_2, [replicate_3], alternative='two-sided')  # Two-sided test to check for differences
                    
                    # Check if the test is significant at 0.05 level
                    if p_val < 0.05:
                        station_significance_count += 1
            # can be untabbed, just shows the compounds where third replicate is significantly different
            #print(f"Station S{i} has {station_significance_count} compounds where third replicate is significantly different.")
            significant_count += station_significance_count

        print(f"Overall, there are {significant_count} instances where the third replicate is significantly different across all stations.")

        # Saving the results 
        options = QFileDialog.Options()
        output_file_name, _ = QFileDialog.getSaveFileName(self, "Save As", "", "Excel Files (*.xlsx);;All Files (*)", options=options)
        
        if not output_file_name:
            return

        with pd.ExcelWriter(output_file_name) as writer:
            # Drop additional columns before saving
            cleaned_df_output = self.drop_additional_columns(df_output, sheet_name="Area")
            cleaned_high_variance_compounds = self.drop_additional_columns(high_variance_compounds, sheet_name="High Variance Compounds")

            cleaned_df_output.to_excel(writer, sheet_name="Area", index=False)
            cleaned_high_variance_compounds.to_excel(writer, sheet_name="High Variance Compounds", index=False)  # Save the high variance compounds to a new sheet
 
        QMessageBox.information(self, "Done", f"Processed file saved as {output_file_name}")

    def drop_additional_columns(self, df, sheet_name=None):
        # List of column name patterns to drop
        cols_to_drop_patterns = ["Variance_S", "Mean_S", "StdDev_S", "Avg_Variance"]

        # For sheet 'Area', drop columns that match "Area %_S" pattern
        if sheet_name == 'Area':
            drop_area_percent_pattern = "Area %_S"
            cols_to_drop = [col for col in df.columns if drop_area_percent_pattern in col]
            df = df.drop(columns=cols_to_drop)

        # Determine columns that match the patterns and drop them
        for pattern in cols_to_drop_patterns:
            cols_to_drop = [col for col in df.columns if pattern in col]
            df = df.drop(columns=cols_to_drop)
        return df


    def process_all(self):
        self.generate_master_list()
        self.process_file()

if __name__ == '__main__':
    app = QApplication(sys.argv)
    ex = ExcelProcessor()
    sys.exit(app.exec_())
